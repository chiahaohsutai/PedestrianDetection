{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retina Net\n",
    "\n",
    "Looking into another pre-trained model, this time we look at a network which tries to balance both accuracy and speed. This model tries to be a middle ground between the Faster RCNN model with ResNet50 and the Faster RCNN model with MobileNet V3 backbones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import RetinaNet_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os import path\n",
    "import numpy as np\n",
    "from torch import device\n",
    "from torch import cuda\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = path.join\n",
    "\n",
    "# image path and annotations path.\n",
    "img_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames'\n",
    "ann_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations'\n",
    "\n",
    "# get the image names.\n",
    "img_names = sorted(list(listdir(img_path)))\n",
    "img_names = [join(img_path, name) for name in img_names]\n",
    "\n",
    "# get the annoation names.\n",
    "ann_names = sorted(list(listdir(ann_path)))\n",
    "ann_names = [join(ann_path, name) for name in ann_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# define the torchvision image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def predict(image, model, device, detection_threshold):\n",
    "\n",
    "    # transform the image to tensor\n",
    "    image = transform(image).to(device)\n",
    "    image = image.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(image)     # get the predictions on the image\n",
    "\n",
    "    # get all the predicited class names\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "\n",
    "    # get score for all the predicted objects\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "    # get all the predicted bounding boxes\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # get boxes above the threshold score\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "    labels = pred_labels[pred_scores >= detection_threshold]\n",
    "    \n",
    "    # keep only pedestrian predictions.\n",
    "    boxes = boxes[labels == 1]\n",
    "    labels = labels[labels == 1]\n",
    "\n",
    "    return boxes, labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, labels, image):\n",
    "\n",
    "    # create a color for the bounding box.\n",
    "    COLOR = [255, 0, 0] \n",
    "  \n",
    "    # read the image with OpenCV\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # draw only the boxes which are persons.\n",
    "    for i, box in enumerate(boxes):\n",
    "        if labels[i] == 1:\n",
    "            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), COLOR, 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model.\n",
    "weights = RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "model = retinanet_resnet50_fpn(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device.\n",
    "device = device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W MPSFallback.mm:11] Warning: The operator 'torchvision::nms' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (function operator())\n"
     ]
    }
   ],
   "source": [
    "# make a prediction.\n",
    "image = Image.open(img_names[0])\n",
    "model.eval().to(device)\n",
    "boxes, labels, _ = predict(image, model, device, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = draw_boxes(boxes, labels, image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# display the image.\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyWindow('Image')\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not as accurate as the Faster RCNN with ResNet50 backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to evaluate the models performance using RSME, MAE, UOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the `Detection` object\n",
    "Detection = namedtuple(\"Detection\", [\"image_path\", \"gt\", \"pred\"])\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"Calculates Intersection Over Union.\"\"\"\n",
    "    \n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def get_boxes(ann):\n",
    "    \"\"\"Gets the correct key for annotations.\"\"\"\n",
    "    \n",
    "    keys = ann.keys()\n",
    "    key = None\n",
    "    \n",
    "    # get the correct key for the bouding box.\n",
    "    for k in ['box_new', 'anno_file', 'anno_previous']:\n",
    "        if k in keys:\n",
    "            key = k\n",
    "            break\n",
    "    if key is None:\n",
    "        raise ValueError(\"Invalid Annotation Error\")\n",
    "    \n",
    "    # get the bounding boxes and convert to coordinates.\n",
    "    bbox = [box[1:] for box in ann[key]]\n",
    "    for box in bbox:\n",
    "        xmin, ymin, w, h = box\n",
    "        xmax, ymax = xmin+w, ymin+h\n",
    "        box[-2] = xmax\n",
    "        box[-1] = ymax\n",
    "        \n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iou = []\n",
    "# truth, pred\n",
    "total_count = []\n",
    "total = len(img_names)\n",
    "\n",
    "# evaluate the model.\n",
    "for img, ann in zip(img_names, ann_names):\n",
    "    \n",
    "    # get appropiate data.\n",
    "    data = loadmat(ann)\n",
    "    photo = Image.open(img)\n",
    "    gt = get_boxes(data)\n",
    "    \n",
    "    # get a prediction\n",
    "    with torch.no_grad():\n",
    "        pred = predict(photo, model, device, 0.7)\n",
    "    \n",
    "    # go through the prediciton and calulate iou\n",
    "    boxes, labels, _ = pred\n",
    "    for bbox in boxes:\n",
    "        # get the iou.\n",
    "        union = 0\n",
    "        for b in gt:\n",
    "            union = max(union, iou(b, bbox))\n",
    "        if union > 0.25:\n",
    "            total_iou.append(union)\n",
    "    \n",
    "    # get a count of the boxes.\n",
    "    total_count.append((len(gt), len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the counts.\n",
    "y_true = [c[0] for c in total_count]\n",
    "y_pred = [c[1] for c in total_count]\n",
    "\n",
    "mse_eval = MSE(y_true, y_pred, squared=False)\n",
    "mape_eval = MAPE(y_true, y_pred)\n",
    "avg_iou = sum(total_iou) / len(total_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.7657859439920796, MAPE: 0.3239119047619048, Avg. IOU: 0.8153919267537796\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE: {mse_eval}, MAPE: {mape_eval}, Avg. IOU: {avg_iou}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
