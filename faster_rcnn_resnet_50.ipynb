{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster RCNN\n",
    "\n",
    "We will be looking into the Faster RCNN Model available in Torch. This model is trained in the MS COCO dataset (which is a common public access database with over 80 classes). In this notebook we will use the model with our data and we are going to just exctract all the cases of a pedestrian which the model detects.\n",
    "\n",
    "We are going to use OpenCV and Numpy to process our images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os import path\n",
    "import numpy as np\n",
    "from torch import device\n",
    "from torch import cuda\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the image and annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = path.join\n",
    "\n",
    "# image path and annotations path.\n",
    "img_path, ann_path = 'PRW/frames', 'PRW/annotations'\n",
    "\n",
    "# get the image names.\n",
    "img_names = sorted(list(listdir(img_path)))\n",
    "img_names = [join(img_path, name) for name in img_names]\n",
    "\n",
    "# get the annoation names.\n",
    "ann_names = sorted(list(listdir(ann_path)))\n",
    "ann_names = [join(ann_path, name) for name in ann_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# define the torchvision image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def predict(image, model, device, detection_threshold):\n",
    "\n",
    "    # transform the image to tensor\n",
    "    image = transform(image).to(device)\n",
    "    image = image.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(image)     # get the predictions on the image\n",
    "\n",
    "    # get all the predicited class names\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "\n",
    "    # get score for all the predicted objects\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "    # get all the predicted bounding boxes\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # get boxes above the threshold score\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "    labels = pred_labels[pred_scores >= detection_threshold]\n",
    "\n",
    "\n",
    "    return boxes, labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, labels, image):\n",
    "\n",
    "    # create a color for the bounding box.\n",
    "    COLOR = [255, 0, 0] \n",
    "  \n",
    "    # read the image with OpenCV\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # draw only the boxes which are persons.\n",
    "    for i, box in enumerate(boxes):\n",
    "        if labels[i] == 1:\n",
    "            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), COLOR, 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model.\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device.\n",
    "device = device('cuda' if cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction.\n",
    "image = Image.open(img_names[0])\n",
    "model.eval().to(device)\n",
    "boxes, labels, output = predict(image, model, device, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[8.9638e-01, 6.5949e+02, 1.6961e+02, 8.8538e+02],\n",
       "         [1.9116e+00, 5.1553e+02, 7.7712e+01, 5.9065e+02],\n",
       "         [1.1603e+02, 6.3123e+02, 2.8724e+02, 8.4227e+02],\n",
       "         [1.1942e+03, 4.5441e+02, 1.2893e+03, 5.1249e+02],\n",
       "         [8.3765e+02, 5.2466e+02, 9.2328e+02, 6.2705e+02],\n",
       "         [3.6395e+02, 5.8677e+02, 5.1083e+02, 7.5238e+02],\n",
       "         [4.6648e+02, 5.6279e+02, 5.9832e+02, 7.3327e+02],\n",
       "         [1.3083e+03, 4.5091e+02, 1.3507e+03, 4.9087e+02],\n",
       "         [2.5544e+02, 5.9312e+02, 4.1038e+02, 7.7712e+02],\n",
       "         [9.7126e+02, 4.9954e+02, 1.0314e+03, 5.9224e+02],\n",
       "         [5.6701e+02, 5.6997e+02, 6.4556e+02, 7.1699e+02],\n",
       "         [1.1979e+03, 4.3838e+02, 1.2316e+03, 4.8274e+02],\n",
       "         [2.5808e+02, 4.9751e+02, 3.2825e+02, 5.5663e+02],\n",
       "         [3.2818e+02, 4.8907e+02, 4.3496e+02, 5.4716e+02],\n",
       "         [7.5894e+02, 5.4281e+02, 8.5454e+02, 6.4946e+02],\n",
       "         [6.5781e+02, 5.3686e+02, 7.7704e+02, 6.7708e+02],\n",
       "         [1.0600e+03, 4.9029e+02, 1.1190e+03, 5.7261e+02],\n",
       "         [9.1200e+02, 5.2250e+02, 9.7980e+02, 6.0507e+02],\n",
       "         [6.1625e+01, 5.1294e+02, 1.7489e+02, 5.7089e+02],\n",
       "         [1.6928e+02, 5.4714e+02, 2.8918e+02, 6.4791e+02],\n",
       "         [1.3275e+03, 4.3003e+02, 1.3497e+03, 4.8113e+02],\n",
       "         [1.0357e+03, 4.2004e+02, 1.0789e+03, 5.6506e+02],\n",
       "         [7.0081e+02, 5.6118e+02, 7.8324e+02, 6.7335e+02],\n",
       "         [1.9585e+02, 4.8693e+02, 2.5973e+02, 6.0144e+02],\n",
       "         [5.5570e+02, 5.3740e+02, 6.8208e+02, 7.1012e+02],\n",
       "         [9.0756e+02, 5.3938e+02, 9.5043e+02, 6.1078e+02],\n",
       "         [2.9970e+02, 6.3437e+02, 4.2707e+02, 7.8407e+02],\n",
       "         [6.4173e+02, 5.5976e+02, 7.3819e+02, 6.8326e+02],\n",
       "         [7.9238e+02, 5.3609e+02, 8.7143e+02, 6.3294e+02],\n",
       "         [5.1622e+02, 5.6134e+02, 6.2770e+02, 7.2923e+02],\n",
       "         [1.2508e+03, 4.5492e+02, 1.3008e+03, 5.0460e+02],\n",
       "         [6.2587e+02, 5.8639e+02, 7.1209e+02, 6.9065e+02],\n",
       "         [9.4002e+02, 5.2854e+02, 9.8484e+02, 6.0353e+02],\n",
       "         [9.2198e+02, 5.3664e+02, 9.6331e+02, 6.0852e+02],\n",
       "         [4.5115e+02, 4.4423e+02, 5.5802e+02, 4.7332e+02],\n",
       "         [8.7422e+02, 5.2110e+02, 9.3689e+02, 6.1460e+02],\n",
       "         [1.6811e+02, 5.6560e+02, 2.4690e+02, 6.4176e+02],\n",
       "         [1.2637e+02, 5.2014e+02, 1.7439e+02, 5.7213e+02],\n",
       "         [1.0709e+01, 5.0792e+02, 1.8478e+02, 5.8736e+02],\n",
       "         [1.4710e+03, 4.4196e+02, 1.5255e+03, 4.7234e+02],\n",
       "         [4.3771e+02, 4.7774e+02, 5.0895e+02, 5.3817e+02],\n",
       "         [2.6197e+02, 5.6855e+02, 6.4459e+02, 7.3689e+02],\n",
       "         [1.8805e+02, 4.7648e+02, 2.4373e+02, 5.6552e+02],\n",
       "         [1.0835e+03, 4.9150e+02, 1.1267e+03, 5.6066e+02],\n",
       "         [9.5578e+02, 4.8491e+02, 1.1171e+03, 5.8744e+02],\n",
       "         [9.2260e+02, 5.0065e+02, 1.0166e+03, 6.0183e+02],\n",
       "         [2.4399e+02, 5.6931e+02, 3.6975e+02, 6.3874e+02],\n",
       "         [9.1783e+02, 4.9622e+02, 9.8237e+02, 5.5835e+02],\n",
       "         [1.0475e+03, 4.9144e+02, 1.0972e+03, 5.8216e+02],\n",
       "         [1.3053e+03, 4.5420e+02, 1.3326e+03, 4.8750e+02],\n",
       "         [1.8557e+02, 5.0889e+02, 2.8211e+02, 5.9387e+02],\n",
       "         [1.0557e+03, 4.4207e+02, 1.0898e+03, 5.5291e+02],\n",
       "         [4.9943e+01, 6.5610e+02, 2.4852e+02, 8.7642e+02],\n",
       "         [1.7158e+02, 5.8965e+02, 5.0134e+02, 7.8224e+02],\n",
       "         [1.4717e+03, 4.4065e+02, 1.5261e+03, 4.7428e+02],\n",
       "         [8.7776e+02, 4.5348e+02, 8.9871e+02, 4.7363e+02],\n",
       "         [1.2707e+03, 4.5153e+02, 1.2936e+03, 4.9803e+02],\n",
       "         [1.4740e+03, 4.4240e+02, 1.5259e+03, 4.7346e+02],\n",
       "         [3.2545e+02, 5.0528e+02, 3.7000e+02, 5.5152e+02],\n",
       "         [4.2325e+02, 5.7646e+02, 5.2590e+02, 7.2931e+02],\n",
       "         [6.6108e+02, 5.1951e+02, 7.3785e+02, 6.0982e+02],\n",
       "         [1.2383e+03, 4.5043e+02, 1.2637e+03, 4.9750e+02],\n",
       "         [6.1709e+02, 5.0481e+02, 7.3144e+02, 6.9181e+02],\n",
       "         [9.1031e+02, 5.0512e+02, 9.5582e+02, 5.6669e+02],\n",
       "         [4.5729e+02, 5.6323e+02, 5.4117e+02, 6.9723e+02],\n",
       "         [8.9343e+02, 4.9843e+02, 1.0745e+03, 5.9826e+02],\n",
       "         [7.9272e+02, 4.6171e+02, 8.3265e+02, 5.0876e+02],\n",
       "         [8.0728e+02, 5.3261e+02, 8.6539e+02, 5.9534e+02],\n",
       "         [1.0605e+03, 4.4976e+02, 1.0851e+03, 4.8946e+02],\n",
       "         [8.1150e+02, 5.0475e+02, 9.0311e+02, 6.1124e+02],\n",
       "         [9.4409e+02, 4.9950e+02, 9.9247e+02, 5.5009e+02],\n",
       "         [1.2273e+03, 4.6084e+02, 1.2751e+03, 5.1340e+02],\n",
       "         [1.8772e+02, 5.0792e+02, 2.9458e+02, 5.7361e+02],\n",
       "         [1.0994e+03, 4.8474e+02, 1.1239e+03, 5.2464e+02],\n",
       "         [1.0639e+03, 4.6445e+02, 1.1002e+03, 5.6189e+02],\n",
       "         [7.8655e+02, 4.6627e+02, 8.3917e+02, 5.0970e+02],\n",
       "         [1.6064e+02, 6.0680e+02, 3.5452e+02, 8.0900e+02],\n",
       "         [7.3577e+02, 4.3451e+02, 7.4856e+02, 4.5425e+02],\n",
       "         [5.3407e+02, 4.7193e+02, 6.5649e+02, 5.2580e+02],\n",
       "         [9.0893e+02, 4.5170e+02, 9.2386e+02, 4.8361e+02],\n",
       "         [1.5240e+02, 4.9312e+02, 3.6743e+02, 5.6607e+02],\n",
       "         [9.9550e+02, 4.7673e+02, 1.0491e+03, 5.8970e+02],\n",
       "         [6.0767e+02, 4.7563e+02, 6.6578e+02, 5.1813e+02],\n",
       "         [1.0423e+03, 4.1873e+02, 1.0641e+03, 5.5021e+02],\n",
       "         [7.8720e+02, 4.6730e+02, 8.3888e+02, 5.0959e+02],\n",
       "         [1.0983e+03, 5.0236e+02, 1.1321e+03, 5.5865e+02],\n",
       "         [4.8415e+02, 4.6616e+02, 8.5390e+02, 5.2372e+02],\n",
       "         [9.1704e+02, 4.6013e+02, 9.3550e+02, 4.8237e+02],\n",
       "         [9.1281e+02, 4.5670e+02, 9.2835e+02, 4.8328e+02],\n",
       "         [1.2055e+03, 4.3843e+02, 1.2566e+03, 4.9800e+02],\n",
       "         [2.7480e+02, 4.7164e+02, 4.9222e+02, 5.5076e+02],\n",
       "         [6.3545e+02, 5.1055e+02, 8.3116e+02, 6.4676e+02],\n",
       "         [6.9600e+02, 4.7420e+02, 7.5206e+02, 5.0706e+02],\n",
       "         [1.0449e+03, 4.1760e+02, 1.0603e+03, 4.6892e+02],\n",
       "         [1.2628e+03, 4.5139e+02, 1.2917e+03, 4.9823e+02],\n",
       "         [1.0608e+03, 4.6005e+02, 1.0888e+03, 4.9852e+02],\n",
       "         [6.1721e+02, 5.3925e+02, 6.9789e+02, 6.3394e+02],\n",
       "         [5.2561e+02, 5.2251e+02, 7.4096e+02, 6.4968e+02],\n",
       "         [4.6622e+02, 5.4238e+02, 7.2798e+02, 7.1475e+02],\n",
       "         [8.3884e+00, 6.5634e+02, 1.1430e+02, 7.5951e+02]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " 'labels': tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  2,\n",
       "          2,  2,  2,  3,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2, 19,  1,  2,  2,\n",
       "          4,  1,  1,  2,  2,  2,  2,  1,  2,  2,  2,  2,  1,  2,  1,  2,  2,  2,\n",
       "          2,  2,  1,  4,  2,  1,  2,  1,  2,  2,  2,  1,  2,  2,  2,  1,  1,  1,\n",
       "          2,  2,  2,  1,  2,  2,  2,  2,  2,  2]),\n",
       " 'scores': tensor([0.9874, 0.9821, 0.9802, 0.9695, 0.9644, 0.9636, 0.9558, 0.9507, 0.9361,\n",
       "         0.9227, 0.9154, 0.8968, 0.8703, 0.8656, 0.8588, 0.8526, 0.8521, 0.8397,\n",
       "         0.8306, 0.8242, 0.8191, 0.8141, 0.8094, 0.7290, 0.7092, 0.6875, 0.6872,\n",
       "         0.6726, 0.6535, 0.6455, 0.6413, 0.6342, 0.6295, 0.6282, 0.5871, 0.5818,\n",
       "         0.5434, 0.4937, 0.4889, 0.4809, 0.4511, 0.4498, 0.4422, 0.4325, 0.4130,\n",
       "         0.4045, 0.3921, 0.3814, 0.3713, 0.3525, 0.3462, 0.3417, 0.3318, 0.3283,\n",
       "         0.3257, 0.3245, 0.3197, 0.3170, 0.3167, 0.3010, 0.2748, 0.2674, 0.2585,\n",
       "         0.2541, 0.2529, 0.2526, 0.2429, 0.2426, 0.2309, 0.2282, 0.2282, 0.2273,\n",
       "         0.2239, 0.2123, 0.2110, 0.2054, 0.1897, 0.1849, 0.1657, 0.1610, 0.1566,\n",
       "         0.1466, 0.1391, 0.1391, 0.1342, 0.1322, 0.1267, 0.1222, 0.1209, 0.1207,\n",
       "         0.1196, 0.1161, 0.1145, 0.1143, 0.1069, 0.1018, 0.1002, 0.0978, 0.0949,\n",
       "         0.0948], grad_fn=<IndexBackward0>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,\n",
       "         2,  2,  1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  2,\n",
       "         2,  2,  2,  3,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2, 19,  1,  2,  2,\n",
       "         4,  1,  1,  2,  2,  2,  2,  1,  2,  2,  2,  2,  1,  2,  1,  2,  2,  2,\n",
       "         2,  2,  1,  4,  2,  1,  2,  1,  2,  2,  2,  1,  2,  2,  2,  1,  1,  1,\n",
       "         2,  2,  2,  1,  2,  2,  2,  2,  2,  2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9874, 0.9821, 0.9802, 0.9695, 0.9644, 0.9636, 0.9558, 0.9507, 0.9361,\n",
       "        0.9227, 0.9154, 0.8968, 0.8703, 0.8656, 0.8588, 0.8526, 0.8521, 0.8397,\n",
       "        0.8306, 0.8242, 0.8191, 0.8141, 0.8094, 0.7290, 0.7092, 0.6875, 0.6872,\n",
       "        0.6726, 0.6535, 0.6455, 0.6413, 0.6342, 0.6295, 0.6282, 0.5871, 0.5818,\n",
       "        0.5434, 0.4937, 0.4889, 0.4809, 0.4511, 0.4498, 0.4422, 0.4325, 0.4130,\n",
       "        0.4045, 0.3921, 0.3814, 0.3713, 0.3525, 0.3462, 0.3417, 0.3318, 0.3283,\n",
       "        0.3257, 0.3245, 0.3197, 0.3170, 0.3167, 0.3010, 0.2748, 0.2674, 0.2585,\n",
       "        0.2541, 0.2529, 0.2526, 0.2429, 0.2426, 0.2309, 0.2282, 0.2282, 0.2273,\n",
       "        0.2239, 0.2123, 0.2110, 0.2054, 0.1897, 0.1849, 0.1657, 0.1610, 0.1566,\n",
       "        0.1466, 0.1391, 0.1391, 0.1342, 0.1322, 0.1267, 0.1222, 0.1209, 0.1207,\n",
       "        0.1196, 0.1161, 0.1145, 0.1143, 0.1069, 0.1018, 0.1002, 0.0978, 0.0949,\n",
       "        0.0948], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = draw_boxes(boxes, labels, image)\n",
    "\n",
    "# display the image.\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Image')\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
