{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster RCNN\n",
    "\n",
    "We will be looking into the Faster RCNN Model available in Torch. This model is trained in the MS COCO dataset (which is a common public access database with over 80 classes). In this notebook we will use the model with our data and we are going to just exctract all the cases of a pedestrian which the model detects.\n",
    "\n",
    "We are going to use OpenCV and Numpy to process our images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os import path\n",
    "import numpy as np\n",
    "from torch import device\n",
    "from torch import cuda\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the image and annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = path.join\n",
    "\n",
    "# image path and annotations path.\n",
    "img_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames'\n",
    "ann_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations'\n",
    "\n",
    "# get the image names.\n",
    "img_names = sorted(list(listdir(img_path)))\n",
    "img_names = [join(img_path, name) for name in img_names]\n",
    "\n",
    "# get the annoation names.\n",
    "ann_names = sorted(list(listdir(ann_path)))\n",
    "ann_names = [join(ann_path, name) for name in ann_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# define the torchvision image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def predict(image, model, device, detection_threshold):\n",
    "\n",
    "    # transform the image to tensor\n",
    "    image = transform(image).to(device)\n",
    "    image = image.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(image)     # get the predictions on the image\n",
    "\n",
    "    # get all the predicited class names\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "\n",
    "    # get score for all the predicted objects\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "    # get all the predicted bounding boxes\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # get boxes above the threshold score\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "    labels = pred_labels[pred_scores >= detection_threshold]\n",
    "    \n",
    "    # keep only pedestrian predictions.\n",
    "    boxes = boxes[labels == 1]\n",
    "    labels = labels[labels == 1]\n",
    "\n",
    "    return boxes, labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, labels, image):\n",
    "\n",
    "    # create a color for the bounding box.\n",
    "    COLOR = [255, 0, 0] \n",
    "  \n",
    "    # read the image with OpenCV\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # draw only the boxes which are persons.\n",
    "    for i, box in enumerate(boxes):\n",
    "        if labels[i] == 1:\n",
    "            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), COLOR, 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model.\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device.\n",
    "device = device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W MPSFallback.mm:11] Warning: The operator 'torchvision::nms' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (function operator())\n"
     ]
    }
   ],
   "source": [
    "# make a prediction.\n",
    "image = Image.open(img_names[0])\n",
    "model.eval().to(device)\n",
    "boxes, labels, output = predict(image, model, device, 0.7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2,  2,  2,\n",
       "         2,  2,  1,  1,  2,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  2,\n",
       "         2,  2,  2,  3,  2,  2,  1,  2,  2,  2,  2,  2,  2,  2, 19,  1,  2,  2,\n",
       "         4,  1,  1,  2,  2,  2,  2,  1,  2,  2,  2,  2,  1,  2,  1,  2,  2,  2,\n",
       "         2,  2,  1,  4,  2,  1,  2,  1,  2,  2,  2,  1,  2,  2,  2,  1,  1,  1,\n",
       "         2,  2,  2,  1,  2,  2,  2,  2,  2,  2], device='mps:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9874, 0.9821, 0.9802, 0.9695, 0.9644, 0.9636, 0.9558, 0.9507, 0.9361,\n",
       "        0.9227, 0.9154, 0.8968, 0.8703, 0.8656, 0.8588, 0.8526, 0.8521, 0.8397,\n",
       "        0.8306, 0.8241, 0.8190, 0.8141, 0.8094, 0.7290, 0.7092, 0.6875, 0.6872,\n",
       "        0.6726, 0.6535, 0.6455, 0.6414, 0.6342, 0.6295, 0.6283, 0.5871, 0.5818,\n",
       "        0.5434, 0.4937, 0.4889, 0.4810, 0.4511, 0.4498, 0.4422, 0.4325, 0.4130,\n",
       "        0.4045, 0.3921, 0.3814, 0.3713, 0.3525, 0.3462, 0.3417, 0.3318, 0.3283,\n",
       "        0.3257, 0.3245, 0.3197, 0.3169, 0.3167, 0.3010, 0.2748, 0.2674, 0.2585,\n",
       "        0.2541, 0.2529, 0.2526, 0.2429, 0.2426, 0.2309, 0.2282, 0.2282, 0.2273,\n",
       "        0.2239, 0.2123, 0.2110, 0.2054, 0.1897, 0.1849, 0.1657, 0.1610, 0.1566,\n",
       "        0.1465, 0.1391, 0.1391, 0.1342, 0.1322, 0.1267, 0.1222, 0.1209, 0.1207,\n",
       "        0.1196, 0.1161, 0.1145, 0.1143, 0.1069, 0.1018, 0.1002, 0.0978, 0.0949,\n",
       "        0.0948], device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1197,  438, 1231,  482],\n",
       "       [1327,  430, 1349,  481],\n",
       "       [1035,  420, 1078,  565],\n",
       "       [ 195,  486,  259,  601]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = draw_boxes(boxes, labels, image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# display the image.\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Image')\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to evaluate the model performance using MAE, IOU and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the `Detection` object\n",
    "Detection = namedtuple(\"Detection\", [\"image_path\", \"gt\", \"pred\"])\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"Calculates Intersection Over Union.\"\"\"\n",
    "    \n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def get_boxes(ann):\n",
    "    \"\"\"Gets the correct key for annotations.\"\"\"\n",
    "    \n",
    "    keys = ann.keys()\n",
    "    key = None\n",
    "    \n",
    "    # get the correct key for the bouding box.\n",
    "    for k in ['box_new', 'anno_file', 'anno_previous']:\n",
    "        if k in keys:\n",
    "            key = k\n",
    "            break\n",
    "    if key is None:\n",
    "        raise ValueError(\"Invalid Annotation Error\")\n",
    "    \n",
    "    # get the bounding boxes and convert to coordinates.\n",
    "    bbox = [box[1:] for box in ann[key]]\n",
    "    for box in bbox:\n",
    "        xmin, ymin, w, h = box\n",
    "        xmax, ymax = xmin+w, ymin+h\n",
    "        box[-2] = xmax\n",
    "        box[-1] = ymax\n",
    "        \n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iou = []\n",
    "# truth, pred\n",
    "total_count = []\n",
    "total = len(img_names)\n",
    "\n",
    "# evaluate the model.\n",
    "for img, ann in zip(img_names, ann_names):\n",
    "    \n",
    "    # get appropiate data.\n",
    "    data = loadmat(ann)\n",
    "    photo = Image.open(img)\n",
    "    gt = get_boxes(data)\n",
    "    \n",
    "    # get a prediction\n",
    "    with torch.no_grad():\n",
    "        pred = predict(photo, model, device, 0.7)\n",
    "    \n",
    "    # go through the prediciton and calulate iou\n",
    "    boxes, labels, _ = pred\n",
    "    for bbox in boxes:\n",
    "        # get the iou.\n",
    "        union = 0\n",
    "        for b in gt:\n",
    "            union = max(union, iou(b, bbox))\n",
    "        if union > 0.25:\n",
    "            total_iou.append(union)\n",
    "    \n",
    "    # get a count of the boxes.\n",
    "    total_count.append((len(gt), len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the counts.\n",
    "y_true = [c[0] for c in total_count]\n",
    "y_pred = [c[1] for c in total_count]\n",
    "\n",
    "mse_eval = MSE(y_true, y_pred, squared=False)\n",
    "mae_eval = MAPE(y_true, y_pred)\n",
    "avg_iou = sum(total_iou) / len(total_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.7832132932300984, MAPE: 0.8232211789430309, Avg. IOU: 0.7728651640533937\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE: {mse_eval}, MAPE: {mae_eval}, Avg. IOU: {avg_iou}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
