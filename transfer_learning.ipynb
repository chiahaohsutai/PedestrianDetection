{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e3a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from scipy.io import loadmat\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms as T\n",
    "from utils import datsprwtrans as prw\n",
    "from torch import device\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ca0d3",
   "metadata": {},
   "source": [
    "## Transfer Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869be09",
   "metadata": {},
   "source": [
    "Function to generate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c93f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    \"\"\"Create a object detection model.\"\"\"\n",
    "    \n",
    "    # load a model pre-trained on MSCOCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc6287",
   "metadata": {},
   "source": [
    "Lets define our custom dataset which will load in our images and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87aa7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRWTransfer(torch.utils.data.Dataset):\n",
    "    \"\"\"Creates a PRW dataset for a Object Detection model.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_root, ann_root, transforms=None):\n",
    "        \"\"\"Instantiates the dataset.\"\"\"\n",
    "        \n",
    "        # set the root directories for data.\n",
    "        self.root_img = img_root\n",
    "        self.root_ann = ann_root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned.\n",
    "        self.imgs = list(sorted(os.listdir(img_root)))\n",
    "        self.anns = list(sorted(os.listdir(ann_root)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Gets a sample from the dataset.\"\"\"\n",
    "        \n",
    "        # load images and bounding boxes.\n",
    "        img_path = os.path.join(self.root_img, self.imgs[idx])\n",
    "        ann_path = os.path.join(self.root_ann, self.anns[idx])\n",
    "        ann = loadmat(ann_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        # get the height and width of the image\n",
    "        image_width = img.shape[1]\n",
    "        image_height = img.shape[0]\n",
    "        img = cv2.resize(img, (416, 416))\n",
    "        img /= 255.0\n",
    "        \n",
    "        if 'box_new' in ann.keys():\n",
    "            ann = ann['box_new']\n",
    "        elif 'anno_file' in ann.keys():\n",
    "            ann = ann['anno_file']\n",
    "        elif 'anno_previous' in ann.keys():\n",
    "            ann = ann['anno_previous']\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Annotation Error\")\n",
    "\n",
    "        # get bounding box coordinates for each mask.\n",
    "        bbox = [a[1:] for a in ann]\n",
    "        for i in bbox:\n",
    "            xmin = i[0]\n",
    "            xmax = xmin + i[2]\n",
    "            ymin = i[1]\n",
    "            ymax = ymin + i[3]\n",
    "            xmin = (xmin/image_width)*416\n",
    "            xmax = (xmax/image_width)*416\n",
    "            ymin = (ymin/image_height)*416\n",
    "            ymax = (ymax/image_height)*416\n",
    "            i[:] = [xmin, ymin, xmax, ymax]\n",
    "        \n",
    "        # store boxes as a tensor.\n",
    "        boxes = torch.as_tensor(bbox, dtype=torch.float32)\n",
    "        # there is only one class.\n",
    "        labels = torch.ones((len(bbox),), dtype=torch.int32)\n",
    "        \n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.FloatTensor(boxes)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        # apply transform if required.\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = img,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            for b in sample['boxes']:\n",
    "                (x1, y1, x2, y2) = b\n",
    "                for a in range(4):\n",
    "                    b[a] = math.floor(b[a]) if b[a] > 1 else b[a]\n",
    "                    b[a] = math.ceil(b[a]) if b[a] < 0 else b[a]\n",
    "                        \n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Gets the size of the dataset (number of samples).\"\"\"\n",
    "        \n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885336a",
   "metadata": {},
   "source": [
    "Defines the transforms for our image. Most of the preprocessing is built into the existing model, so we just need to convert to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d2343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training tranforms\n",
    "def get_transform():\n",
    "    return A.Compose([ToTensorV2(p=1.0),], \n",
    "                     bbox_params={'format': 'pascal_voc',\n",
    "                                  'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e076f6",
   "metadata": {},
   "source": [
    "Dataloader helper. The dataset will output variable size data so we need to work with a custom data getter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251c4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Dataloader data gatherer.\"\"\"\n",
    "    \n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40977d82",
   "metadata": {},
   "source": [
    "Create a Torch device for memory management and GPU processes and create an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39cb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device.\n",
    "device = device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961dcec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Set\n"
     ]
    }
   ],
   "source": [
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "print('Model Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bd92a",
   "metadata": {},
   "source": [
    "Create a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675abce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    \"\"\"Trains the model.\"\"\"\n",
    "    \n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    \n",
    "    for data in train_data_loader:\n",
    "        # zero oput the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        # move the target data and the image to device.\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # calculate the loss.\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # record the loss. (Tracking)\n",
    "        totalTrainLoss += losses\n",
    "        \n",
    "        # backpropagate the error.\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return totalTrainLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd43816",
   "metadata": {},
   "source": [
    "Create a validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9b2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running validation iterations\n",
    "def validate(valid_data_loader, model):\n",
    "    \"\"\"Validates the model.\"\"\"\n",
    "    \n",
    "    totalValLoss = 0\n",
    "    \n",
    "    for data in valid_data_loader:\n",
    "        # unpack the data.\n",
    "        images, targets = data\n",
    "        \n",
    "        # move images and target data to device. \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # turn off gradient tracking.\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "        # calculate the loss.\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        totalValLoss += losses\n",
    "        \n",
    "    return totalValLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47757c2a",
   "metadata": {},
   "source": [
    "Now lets generate the dataset and load it to a Torch dataloader for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16dafc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 5\n",
    "\n",
    "# define required data directories.\n",
    "img_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames'\n",
    "ann_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations'\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = prw.PRWTransfer(img_path, ann_path, get_transform())\n",
    "dataset_test = prw.PRWTransfer(img_path, ann_path, get_transform())\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-1000])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-1000:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH, shuffle=True, num_workers=4,\n",
    "    collate_fn=prw.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=BATCH, shuffle=False, num_workers=4,\n",
    "    collate_fn=prw.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4967bb3",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f91f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# freeze the backbone of the model.\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "trainSteps = len(dataset) // BATCH\n",
    "valSteps = len(dataset_test) // BATCH\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# for tracking histories.\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "    \n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(data_loader, model)\n",
    "    val_loss = validate(data_loader_test, model)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # print metrics.\n",
    "    print(f\"Epoch #{epoch+1} train loss: {train_loss:.3f}\")   \n",
    "    print(f\"Epoch #{epoch+1} validation loss: {val_loss:.3f}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "    \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = train_loss / trainSteps\n",
    "    avgValLoss = val_loss / valSteps\n",
    "    \n",
    "    train_loss_list.append(avgTrainLoss)\n",
    "    val_loss_list.append(avgValLoss)\n",
    "    \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf2fbc",
   "metadata": {},
   "source": [
    "Convert tensors to values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe4dd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_loss_list)):\n",
    "    train_loss_list[idx] = train_loss_list[idx].detach().cpu().numpy()\n",
    "    val_loss_list[idx] = val_loss_list[idx].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df15e1",
   "metadata": {},
   "source": [
    "Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15525c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving object detector model...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# path to current folder.\n",
    "curr_dir = r'/Users/chiahaohsutai/Documents/GitHub/PedestrianDetection/save'\n",
    "model_dir = os.path.join(curr_dir, 'model_transfer.pth')\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "torch.save(model, model_dir)\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss_list, label=\"total_train_loss\")\n",
    "plt.plot(val_loss_list, label=\"total_val_loss\")\n",
    "plt.title(\"Total Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# save the training plot\n",
    "plotPath = os.path.sep.join([curr_dir, \"training_transfer.png\"])\n",
    "plt.savefig(plotPath)\n",
    "\n",
    "# show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461aaff3",
   "metadata": {},
   "source": [
    "There is an error in loss tracking. I think we might have a bug due to using a Python list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189da07",
   "metadata": {},
   "source": [
    "Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c7de5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in Eval Mode\n"
     ]
    }
   ],
   "source": [
    "# move the model to device.\n",
    "model.eval().to(device)\n",
    "print('Model in Eval Mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fcb6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(photo, model, device, thresh):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(photo.to(device))\n",
    "    \n",
    "    # load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "\n",
    "    # carry further only if there are detected boxes\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        # get the boxes and the scores.\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        labels = outputs[0]['labels'].cpu().numpy()\n",
    "\n",
    "        # filter out boxes according to `detection_threshold`\n",
    "        labels = labels[scores >= thresh].astype(np.int32)\n",
    "        boxes = boxes[scores >= thresh].astype(np.int32)\n",
    "        boxes = boxes[labels == 1]\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cd32be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    \"\"\"Calculates Intersection Over Union.\"\"\"\n",
    "    \n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def get_boxes(ann):\n",
    "    \"\"\"Gets the correct key for annotations.\"\"\"\n",
    "    \n",
    "    keys = ann.keys()\n",
    "    key = None\n",
    "    \n",
    "    # get the correct key for the bouding box.\n",
    "    for k in ['box_new', 'anno_file', 'anno_previous']:\n",
    "        if k in keys:\n",
    "            key = k\n",
    "            break\n",
    "    if key is None:\n",
    "        raise ValueError(\"Invalid Annotation Error\")\n",
    "    \n",
    "    # get the bounding boxes and convert to coordinates.\n",
    "    bbox = [box[1:] for box in ann[key]]\n",
    "    for box in bbox:\n",
    "        xmin, ymin, w, h = box\n",
    "        xmax, ymax = xmin+w, ymin+h\n",
    "        box[-2] = xmax\n",
    "        box[-1] = ymax\n",
    "        \n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27ce0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1bffe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image names.\n",
    "images_names = sorted(list(os.listdir(img_path)))\n",
    "images_names = [os.path.join(img_path, name) for name in images_names]\n",
    "\n",
    "# get the annoation names.\n",
    "ann_names = sorted(list(os.listdir(ann_path)))\n",
    "ann_names = [os.path.join(ann_path, name) for name in ann_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932bd510",
   "metadata": {},
   "source": [
    "Calculate the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f98f8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iou = []\n",
    "# truth, pred\n",
    "total_count = []\n",
    "total = len(images_names)\n",
    "\n",
    "# evaluate the model.\n",
    "for idx in indices[-1000:]:\n",
    "    image, ann = images_names[idx], ann_names[idx]\n",
    "    \n",
    "    # get appropiate data.\n",
    "    data = loadmat(ann)\n",
    "    photo = cv2.imread(image)\n",
    "    photo = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    # make the pixel range between 0 and 1\n",
    "    photo /= 255.0\n",
    "    \n",
    "    # bring color channels to front\n",
    "    photo = np.transpose(photo, (2, 0, 1)).astype(np.float32)\n",
    "    # convert to tensor\n",
    "    photo = torch.tensor(photo, dtype=torch.float)\n",
    "    # add batch dimension\n",
    "    photo = torch.unsqueeze(photo, 0)\n",
    "    \n",
    "    gt = get_boxes(data)\n",
    "    \n",
    "    # get a prediction\n",
    "    pred = predict(photo, model, device, 0.7)\n",
    "    \n",
    "    # go through the prediciton and calulate iou\n",
    "    boxes = pred\n",
    "    for bbox in boxes:\n",
    "        # get the iou.\n",
    "        union = 0\n",
    "        for b in gt:\n",
    "            union = max(union, iou(b, bbox))\n",
    "        if union > 0.25:\n",
    "            total_iou.append(union)\n",
    "    \n",
    "    # get a count of the boxes.\n",
    "    total_count.append((len(gt), len(boxes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3377bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c250b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the counts.\n",
    "y_true = [c[0] for c in total_count]\n",
    "y_pred = [c[1] for c in total_count]\n",
    "\n",
    "mse_eval = MSE(y_true, y_pred, squared=False)\n",
    "mape_eval = MAPE(y_true, y_pred)\n",
    "avg_iou = sum(total_iou) / len(total_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49417c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.094386400915282, MAPE: 1.126253651903652, Avg. IOU: 0.6601183056870884\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE: {mse_eval}, MAPE: {mape_eval}, Avg. IOU: {avg_iou}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
