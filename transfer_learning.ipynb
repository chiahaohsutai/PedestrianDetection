{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e3a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from scipy.io import loadmat\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms as T\n",
    "from utils import datsprwtrans as prw\n",
    "from torch import device\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ca0d3",
   "metadata": {},
   "source": [
    "## Transfer Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869be09",
   "metadata": {},
   "source": [
    "Function to generate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c93f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    \"\"\"Create a object detection model.\"\"\"\n",
    "    \n",
    "    # load a model pre-trained on MSCOCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc6287",
   "metadata": {},
   "source": [
    "Lets define our custom dataset which will load in our images and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87aa7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRWTransfer(torch.utils.data.Dataset):\n",
    "    \"\"\"Creates a PRW dataset for a Object Detection model.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_root, ann_root, transforms=None):\n",
    "        \"\"\"Instantiates the dataset.\"\"\"\n",
    "        \n",
    "        # set the root directories for data.\n",
    "        self.root_img = img_root\n",
    "        self.root_ann = ann_root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned.\n",
    "        self.imgs = list(sorted(os.listdir(img_root)))\n",
    "        self.anns = list(sorted(os.listdir(ann_root)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Gets a sample from the dataset.\"\"\"\n",
    "        \n",
    "        # load images and bounding boxes.\n",
    "        img_path = os.path.join(self.root_img, self.imgs[idx])\n",
    "        ann_path = os.path.join(self.root_ann, self.anns[idx])\n",
    "        ann = loadmat(ann_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        # get the height and width of the image\n",
    "        image_width = img.shape[1]\n",
    "        image_height = img.shape[0]\n",
    "        img = cv2.resize(img, (416, 416))\n",
    "        img /= 255.0\n",
    "        \n",
    "        if 'box_new' in ann.keys():\n",
    "            ann = ann['box_new']\n",
    "        elif 'anno_file' in ann.keys():\n",
    "            ann = ann['anno_file']\n",
    "        elif 'anno_previous' in ann.keys():\n",
    "            ann = ann['anno_previous']\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Annotation Error\")\n",
    "\n",
    "        # get bounding box coordinates for each mask.\n",
    "        bbox = [a[1:] for a in ann]\n",
    "        for i in bbox:\n",
    "            xmin = i[0]\n",
    "            xmax = xmin + i[2]\n",
    "            ymin = i[1]\n",
    "            ymax = ymin + i[3]\n",
    "            xmin = (xmin/image_width)*416\n",
    "            xmax = (xmax/image_width)*416\n",
    "            ymin = (ymin/image_height)*416\n",
    "            ymax = (ymax/image_height)*416\n",
    "            i[:] = [xmin, ymin, xmax, ymax]\n",
    "        \n",
    "        # store boxes as a tensor.\n",
    "        boxes = torch.as_tensor(bbox, dtype=torch.float32)\n",
    "        # there is only one class.\n",
    "        labels = torch.ones((len(bbox),), dtype=torch.int32)\n",
    "        \n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.FloatTensor(boxes)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        # apply transform if required.\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = img,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Gets the size of the dataset (number of samples).\"\"\"\n",
    "        \n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885336a",
   "metadata": {},
   "source": [
    "Defines the transforms for our image. Most of the preprocessing is built into the existing model, so we just need to convert to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d2343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training tranforms\n",
    "def get_transform():\n",
    "    return A.Compose([ToTensorV2(p=1.0),], \n",
    "                     bbox_params={'format': 'pascal_voc','label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e076f6",
   "metadata": {},
   "source": [
    "Dataloader helper. The dataset will output variable size data so we need to work with a custom data getter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251c4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Dataloader data gatherer.\"\"\"\n",
    "    \n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40977d82",
   "metadata": {},
   "source": [
    "Create a Torch device for memory management and GPU processes and create an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39cb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device.\n",
    "device = device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961dcec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Set\n"
     ]
    }
   ],
   "source": [
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "print('Model Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bd92a",
   "metadata": {},
   "source": [
    "Create a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675abce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    \"\"\"Trains the model.\"\"\"\n",
    "    \n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    \n",
    "    for data in train_data_loader:\n",
    "        # zero oput the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        # move the target data and the image to device.\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # calculate the loss.\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        # record the loss. (Tracking)\n",
    "        totalTrainLoss += loss_value\n",
    "        \n",
    "        # backpropagate the error.\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return totalTrainLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd43816",
   "metadata": {},
   "source": [
    "Create a validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9b2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running validation iterations\n",
    "def validate(valid_data_loader, model):\n",
    "    \"\"\"Validates the model.\"\"\"\n",
    "    \n",
    "    totalValLoss = 0\n",
    "    \n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        # unpack the data.\n",
    "        images, targets = data\n",
    "        \n",
    "        # move images and target data to device. \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # turn off gradient tracking.\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "        # calculate the loss.\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        totalValLoss += loss_value\n",
    "        \n",
    "    return totalValLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689cd25",
   "metadata": {},
   "source": [
    "Now lets generate the dataset and load it to a Torch dataloader for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c38a2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 5\n",
    "\n",
    "# define required data directories.\n",
    "img_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames'\n",
    "ann_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations'\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = prw.PRWTransfer(img_path, ann_path, get_transform())\n",
    "dataset_test = prw.PRWTransfer(img_path, ann_path, get_transform())\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-1000])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-1000:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH, shuffle=True, num_workers=4,\n",
    "    collate_fn=prw.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=BATCH, shuffle=False, num_workers=4,\n",
    "    collate_fn=prw.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4967bb3",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e166cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W MPSFallback.mm:11] Warning: The operator 'torchvision::nms' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (function operator())\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "trainSteps = len(dataset) // BATCH\n",
    "valSteps = len(dataset_test) // BATCH\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# for tracking histories.\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "    \n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(data_loader, model)\n",
    "    val_loss = validate(data_loader_test, model)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # print metrics.\n",
    "    print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "    \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = train_loss / trainSteps\n",
    "    avgValLoss = val_loss / valSteps\n",
    "    \n",
    "    train_loss_list.append(avgTrainLoss)\n",
    "    val_loss_list.append(avgValLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15525c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to current folder.\n",
    "curr_dir = r'/Users/chiahaohsutai/Documents/GitHub/PedestrianDetection/save'\n",
    "model_dir = os.path.join(curr_dir, 'model_transfer.pth')\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "torch.save(model, model_dir)\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss_list, label=\"total_train_loss\")\n",
    "plt.plot(val_loss_list, label=\"total_val_loss\")\n",
    "plt.title(\"Total Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# save the training plot\n",
    "plotPath = os.path.sep.join([curr_dir, \"training_transfer.png\"])\n",
    "plt.savefig(plotPath)\n",
    "\n",
    "# show the plot.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
