{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f76365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from scipy.io import loadmat\n",
    "import cv2 as cv\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56224b9d",
   "metadata": {},
   "source": [
    "## HourGlass Moudule (HourGlassNet)\n",
    "\n",
    "Creating a smaller version of the hourglass network (encoding/decoding network). The hourglass network is usually used in a stacked fashion. In the CenterNet Model (which is a Object Detection Algorithm/Architecture), they use a stacked hourglass network for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf26e1",
   "metadata": {},
   "source": [
    "Below you can see how the HourGlass Module is being implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd9830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"Creates a ResNet Block for feature extraction.\"\"\"\n",
    "\n",
    "    def __init__(self, inp_dim, out_dim):\n",
    "        \"\"\"Instantiates the Residual Module.\"\"\"\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        # half the output dimension.\n",
    "        out = out_dim//2\n",
    "\n",
    "        # sequence of layers (Batch Normalization, ReLu, Convolution)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(inp_dim)\n",
    "        self.conv1 = nn.Conv2d(inp_dim, out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out)\n",
    "        self.conv2 = nn.Conv2d(out, out, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(out)\n",
    "        self.conv3 = nn.Conv2d(out, out_dim, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # add a skip layer for residual information\n",
    "        self.skip_layer = nn.Conv2d(inp_dim, out_dim, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.need_skip = not (inp_dim == out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines a forward pass of the ResNet block.\"\"\"\n",
    "\n",
    "        # save residual information.\n",
    "        residual = self.skip_layer(x) if self.need_skip else x\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out += residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f266cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hourglass(nn.Module):\n",
    "    \"\"\"Hourglass Module.\"\"\"\n",
    "\n",
    "    def __init__(self, n, filters, bn=None):\n",
    "        \"\"\"Creates an Hourglass Module/Network.\"\"\"\n",
    "\n",
    "        super(Hourglass, self).__init__()\n",
    "        self.n = n\n",
    "        # up-sampling data.\n",
    "        self.up1 = ResNetBlock(filters, filters)\n",
    "        # encoding/feature extraction.\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.low1 = ResNetBlock(filters, filters)\n",
    "        # recursion to add more resnet blocks.\n",
    "        self.low2 = Hourglass(n-1, filters, bn=bn) if self.n > 1 else ResNetBlock(filters, filters)\n",
    "        self.low3 = ResNetBlock(filters, filters)\n",
    "        # up-sampling data.\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the Hourglass Module.\"\"\"\n",
    "\n",
    "        up1  = self.up1(x)\n",
    "        pool1 = self.pool1(x)\n",
    "        # encoding (lower levels).\n",
    "        low1 = self.low1(pool1)\n",
    "        low2 = self.low2(low1)\n",
    "        low3 = self.low3(low2)\n",
    "        up2  = self.up2(low3)\n",
    "\n",
    "        # decoding (up-sampling).\n",
    "        return up1 + up2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8780e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourGlassNetwork(nn.Module):\n",
    "    \"\"\"Creates an Hourglass Network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape=(256, 256, 3), num_stack=1, num_residual=1, num_heatmap=1):\n",
    "        \"\"\"Instantiates the network (we want single center key points.)\"\"\"\n",
    "\n",
    "        super(HourGlassNetwork, self).__init__()\n",
    "\n",
    "        # initial feature extraction layers.\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.res1 = ResNetBlock(64, 128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.res2 = ResNetBlock(128, 128)\n",
    "        self.res3 = ResNetBlock(128, 256)\n",
    "        self.hg1 = Hourglass(4, 256)\n",
    "        self.linear = nn.Conv2d(256, 256, kernel_size=1, padding=0, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.y = nn.Conv2d(256, num_heatmap, kernel_size=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.res1(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.res2(out)\n",
    "        out = self.res3(out)\n",
    "        out = self.hg1(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.y(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61865eea",
   "metadata": {},
   "source": [
    "Create a Torch device to manage the data and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bdd4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Set\n"
     ]
    }
   ],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "DEVICE = torch.device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create the model.\n",
    "model = HourGlassNetwork()\n",
    "model.to(DEVICE)\n",
    "print('Device Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f64a2",
   "metadata": {},
   "source": [
    "Test an output of the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27333e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames/c1s1_000801.jpg'\n",
    "image = cv.imread(img_path)\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "image = cv.resize(image, (256, 256))\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "process = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "img = process(image).unsqueeze(0).to(DEVICE)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8e20f",
   "metadata": {},
   "source": [
    "Run a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85b5a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the model in evalution mode.\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(img)\n",
    "\n",
    "# get the shape of the output. \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce5f5e",
   "metadata": {},
   "source": [
    "Get a look at what the prediction looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e91f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1295, -0.1512, -0.1849,  ...,  0.0416,  0.0409,  0.0465],\n",
       "          [-0.1538, -0.1727, -0.2024,  ...,  0.0381,  0.0267,  0.0582],\n",
       "          [-0.1895, -0.2120, -0.2323,  ...,  0.0290,  0.0436,  0.0566],\n",
       "          ...,\n",
       "          [ 0.0969,  0.0890,  0.0903,  ...,  0.1402,  0.1566,  0.1640],\n",
       "          [ 0.0967,  0.0955,  0.0820,  ...,  0.1465,  0.1495,  0.1527],\n",
       "          [ 0.0977,  0.0975,  0.0862,  ...,  0.1482,  0.1557,  0.1558]]]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00caad0e",
   "metadata": {},
   "source": [
    "Now look at the annotations and boxes for the image. Get the centers for each pedestrian in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af472bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the annotations.\n",
    "ann_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations/c1s1_000801.jpg.mat'\n",
    "annotation = loadmat(ann_path)['box_new']\n",
    "annotation = [ann[1:] for ann in annotation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a507f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the centers.\n",
    "centers = []\n",
    "for ann in annotation:\n",
    "    x, y, w, h = ann\n",
    "    centers.append((x+(w/2), y+(h/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4053965",
   "metadata": {},
   "source": [
    "Run the cell below as code if you want to see the centers being drawn in the image. The centers are red dots and they are pretty small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d022f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "779cbd0e",
   "metadata": {},
   "source": [
    "draw = ImageDraw.Draw(im)\n",
    "for center in centers:\n",
    "    draw.point(center, fill=(255, 0, 0))\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda1db9",
   "metadata": {},
   "source": [
    "Now we are going to focus on training and learning. We need to create a Gaussian Patch to help the model learn. Otherwise the model might have a hard time predicting the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcfb2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch(scale=12):\n",
    "    \"\"\"Creates a heatmap using Gaussian Distribution.\"\"\"\n",
    "\n",
    "    # constants.\n",
    "    sigma = 1\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x_mesh, y_mesh = torch.meshgrid(torch.arange(0, 6*sigma+1, 1), torch.arange(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "    # the center of the gaussian patch should be 1\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    # generate this 7x7 gaussian patch\n",
    "    xmesh = torch.square(torch.sub(x_mesh, center_x))\n",
    "    ymesh = torch.square(torch.sub(y_mesh, center_y))\n",
    "    denom = (sigma**2) * 2\n",
    "    gaussian_patch = torch.mul(torch.exp(torch.div(torch.neg(torch.add(xmesh, ymesh)), denom)), scale)\n",
    "\n",
    "    return gaussian_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05191da3",
   "metadata": {},
   "source": [
    "Display what the Gaussian Patch looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c04135ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2340980e-04, 1.5034392e-03, 6.7379470e-03, 1.1108996e-02,\n",
       "        6.7379470e-03, 1.5034392e-03, 1.2340980e-04],\n",
       "       [1.5034392e-03, 1.8315639e-02, 8.2084998e-02, 1.3533528e-01,\n",
       "        8.2084998e-02, 1.8315639e-02, 1.5034392e-03],\n",
       "       [6.7379470e-03, 8.2084998e-02, 3.6787945e-01, 6.0653067e-01,\n",
       "        3.6787945e-01, 8.2084998e-02, 6.7379470e-03],\n",
       "       [1.1108996e-02, 1.3533528e-01, 6.0653067e-01, 1.0000000e+00,\n",
       "        6.0653067e-01, 1.3533528e-01, 1.1108996e-02],\n",
       "       [6.7379470e-03, 8.2084998e-02, 3.6787945e-01, 6.0653067e-01,\n",
       "        3.6787945e-01, 8.2084998e-02, 6.7379470e-03],\n",
       "       [1.5034392e-03, 1.8315639e-02, 8.2084998e-02, 1.3533528e-01,\n",
       "        8.2084998e-02, 1.8315639e-02, 1.5034392e-03],\n",
       "       [1.2340980e-04, 1.5034392e-03, 6.7379470e-03, 1.1108996e-02,\n",
       "        6.7379470e-03, 1.5034392e-03, 1.2340980e-04]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, h = im.size\n",
    "x, y = centers[0]\n",
    "patch = generate_patch(1)\n",
    "torch.Tensor.numpy(patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9169fa44",
   "metadata": {},
   "source": [
    "Display it as an image."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d93cef0e",
   "metadata": {},
   "source": [
    "# add a dimension. (Grayscale)\n",
    "patch = torch.unsqueeze(patch, dim=0)\n",
    "\n",
    "# convert to image.\n",
    "t = transforms.ToPILImage()\n",
    "patch_img = t(patch)\n",
    "# display.\n",
    "patch_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0accece2",
   "metadata": {},
   "source": [
    "The Gaussian Patch is being created properly as seen by the output tensor after converting from img to tensor. Now we should try to figure out how to place the patch in the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fe8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap(width, height, center_x, center_y, gau_patch):\n",
    "    \"\"\"Places a Gaussian Patch in the heatmap.\"\"\"\n",
    "\n",
    "    # constants.\n",
    "    heatmap = np.zeros((height, width))\n",
    "    sigma = 1\n",
    "    visibility = 2\n",
    "    gau_patch = torch.Tensor.numpy(gau_patch)\n",
    "\n",
    "    # this gaussian patch is 7x7, let's get four corners of it first\n",
    "    xmin = center_x - 3 * sigma\n",
    "    ymin = center_y - 3 * sigma\n",
    "    xmax = center_x + 3 * sigma\n",
    "    ymax = center_y + 3 * sigma\n",
    "    \n",
    "    # if outside the image don't include the gaussian patch.\n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    # determine boundaries for patch if outside the image.\n",
    "    patch_xmin = max(0, -xmin)\n",
    "    patch_ymin = max(0, -ymin)\n",
    "    patch_xmax = min(xmax, width) - xmin\n",
    "    patch_ymax = min(ymax, height) - ymin\n",
    "    \n",
    "    # we need to determine where to put this patch in the whole heatmap\n",
    "    heatmap_xmin = max(0, xmin)\n",
    "    heatmap_ymin = max(0, ymin)\n",
    "\n",
    "    for j in range(patch_ymin, patch_ymax):\n",
    "        for i in range(patch_xmin, patch_xmax):\n",
    "            heatmap[j+heatmap_ymin, i+heatmap_xmin] = gau_patch[j, i]\n",
    "\n",
    "    return torch.FloatTensor(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261627e3",
   "metadata": {},
   "source": [
    "Before generating the final image for training (aka the heatmap), we are going to preprocess the data. To a 64x64 image which would be equivalent to the output shape of the HourGlass Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da8904d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx, ry = 64/w, 64/h\n",
    "\n",
    "# re-locate the centers for the new image dimensions.\n",
    "resized_centers = []\n",
    "for center in centers:\n",
    "    cx, cy = center\n",
    "    resized_centers.append((cx*rx, cy*ry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5132edc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pos, y_pos = resized_centers[0]\n",
    "heatmap_example = make_heatmap(64, 64, int(x_pos), int(y_pos), patch)\n",
    "heatmap_example[int(y_pos)][int(x_pos)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5edc46",
   "metadata": {},
   "source": [
    "The patch was added since the center of the pedestrian has value 1. Run the code below if you want a visualization of the heatmap."
   ]
  },
  {
   "cell_type": "raw",
   "id": "23d2b367",
   "metadata": {},
   "source": [
    "to_img = transforms.ToPILImage()\n",
    "heatmap_img = to_img(torch.unsqueeze(heatmap_example, dim=0))\n",
    "heatmap_img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
