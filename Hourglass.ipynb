{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87b71781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from scipy.io import loadmat\n",
    "import cv2 as cv\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import datasetprw as prw\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cde6c0",
   "metadata": {},
   "source": [
    "## HourGlass Moudule (HourGlassNet)\n",
    "\n",
    "Creating a smaller version of the hourglass network (encoding/decoding network). The hourglass network is usually used in a stacked fashion. In the CenterNet Model (which is a Object Detection Algorithm/Architecture), they use a stacked hourglass network for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5baaf32",
   "metadata": {},
   "source": [
    "Below you can see how the HourGlass Module is being implemented. The Hourglass Module is based on the paper [Stacked Hourglass Networks for Human Estimation](https://arxiv.org/pdf/1603.06937.pdf) and we are using it to create an image that contains center keypoints. Center keypoints refers to the center of a bounding box drawn around a pedestrian. We use this architecture in essence to create a heatmap, which indicates regions which pedestrians are most likely to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9cdd2",
   "metadata": {},
   "source": [
    "We focus on a single HourGlass Module, which means that we use a single HourgGlass for the entire model. We chose to do this because our problem/task is much less complex than detecting human keypoints; hence we believe we won't need to use as many HourGlasses in the model. Therefore, in this case, the HourGlass Module is the Model/Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fd1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"Creates a ResNet Block for feature extraction.\"\"\"\n",
    "\n",
    "    def __init__(self, inp_dim, out_dim):\n",
    "        \"\"\"Instantiates the Residual Module.\"\"\"\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        # half the output dimension.\n",
    "        out = out_dim//2\n",
    "\n",
    "        # sequence of layers (Batch Normalization, ReLu, Convolution)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(inp_dim)\n",
    "        self.conv1 = nn.Conv2d(inp_dim, out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out)\n",
    "        self.conv2 = nn.Conv2d(out, out, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(out)\n",
    "        self.conv3 = nn.Conv2d(out, out_dim, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # add a skip layer for residual information\n",
    "        self.skip_layer = nn.Conv2d(inp_dim, out_dim, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.need_skip = not (inp_dim == out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines a forward pass of the ResNet block.\"\"\"\n",
    "\n",
    "        # save residual information.\n",
    "        residual = self.skip_layer(x) if self.need_skip else x\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out += residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df23c8",
   "metadata": {},
   "source": [
    "We use a recursive approach to generate the decoder/encoder layers in the model. This can be seen when creating the self.low2 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383a6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hourglass(nn.Module):\n",
    "    \"\"\"Hourglass Module.\"\"\"\n",
    "\n",
    "    def __init__(self, n, filters, bn=None):\n",
    "        \"\"\"Creates an Hourglass Module/Network.\"\"\"\n",
    "\n",
    "        super(Hourglass, self).__init__()\n",
    "        self.n = n\n",
    "        # up-sampling data.\n",
    "        self.up1 = ResNetBlock(filters, filters)\n",
    "        # encoding/feature extraction.\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.low1 = ResNetBlock(filters, filters)\n",
    "        # recursion to add more resnet blocks.\n",
    "        self.low2 = Hourglass(n-1, filters, bn=bn) if self.n > 1 else ResNetBlock(filters, filters)\n",
    "        self.low3 = ResNetBlock(filters, filters)\n",
    "        # up-sampling data.\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the Hourglass Module.\"\"\"\n",
    "\n",
    "        up1  = self.up1(x)\n",
    "        pool1 = self.pool1(x)\n",
    "        # encoding (lower levels).\n",
    "        low1 = self.low1(pool1)\n",
    "        low2 = self.low2(low1)\n",
    "        low3 = self.low3(low2)\n",
    "        up2  = self.up2(low3)\n",
    "\n",
    "        # decoding (up-sampling).\n",
    "        return up1 + up2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f1e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourGlassNetwork(nn.Module):\n",
    "    \"\"\"Creates an Hourglass Network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape=(256, 256, 3), num_stack=1, num_heatmap=1):\n",
    "        \"\"\"Instantiates the network (we want single center key points.)\"\"\"\n",
    "\n",
    "        super(HourGlassNetwork, self).__init__()\n",
    "\n",
    "        # initial feature extraction layers.\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.res1 = ResNetBlock(64, 128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.res2 = ResNetBlock(128, 128)\n",
    "        self.res3 = ResNetBlock(128, 256)\n",
    "        self.hg1 = Hourglass(4, 256)\n",
    "        self.linear = nn.Conv2d(256, 256, kernel_size=1, padding=0, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.y = nn.Conv2d(256, num_heatmap, kernel_size=1, padding=0, bias=True)\n",
    "        # additional sigmoid layer to keep values within 0 and 1.\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.res1(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.res2(out)\n",
    "        out = self.res3(out)\n",
    "        out = self.hg1(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.y(out)\n",
    "        out = self.final(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79838a0",
   "metadata": {},
   "source": [
    "Create a Torch device to manage the data and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57779c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Set\n"
     ]
    }
   ],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "DEVICE = torch.device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create the model.\n",
    "model = HourGlassNetwork()\n",
    "model.to(DEVICE)\n",
    "print('Device Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a275a",
   "metadata": {},
   "source": [
    "Test an output of the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb04848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames/c1s1_000801.jpg'\n",
    "image = cv.imread(img_path)\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "image = cv.resize(image, (256, 256))\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "process = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "img = process(image).unsqueeze(0).to(DEVICE)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa1f92",
   "metadata": {},
   "source": [
    "Run a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dbb7a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the model in evalution mode.\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(img)\n",
    "\n",
    "# get the shape of the output. \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162f950",
   "metadata": {},
   "source": [
    "Get a look at what the prediction looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95804eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5221, 0.5262, 0.5267,  ..., 0.5243, 0.5215, 0.5205],\n",
       "          [0.5134, 0.5143, 0.5151,  ..., 0.5213, 0.5181, 0.5180],\n",
       "          [0.5025, 0.5032, 0.5022,  ..., 0.5146, 0.5183, 0.5159],\n",
       "          ...,\n",
       "          [0.5066, 0.5070, 0.5110,  ..., 0.5216, 0.5229, 0.5203],\n",
       "          [0.5059, 0.5077, 0.5042,  ..., 0.5233, 0.5239, 0.5215],\n",
       "          [0.5055, 0.5069, 0.5050,  ..., 0.5235, 0.5221, 0.5192]]]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140c5eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5134, device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_pred = torch.flatten(pred)\n",
    "torch.mean(flat_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a1c04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of squeezing the image to get a 64x64 output.\n",
    "torch.squeeze(pred, (0, 1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158521f",
   "metadata": {},
   "source": [
    "Sigmoid Activation layer allows use to mantain the range of pixel intensities between 0-1, which will help us detemine peaks in the final output. A peak would be a pixel with intensity 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a621c34",
   "metadata": {},
   "source": [
    "Now look at the annotations and boxes for the image. Get the centers for each pedestrian in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8db2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the annotations.\n",
    "ann_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations/c1s1_000801.jpg.mat'\n",
    "annotation = loadmat(ann_path)['box_new']\n",
    "annotation = [ann[1:] for ann in annotation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f62a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the centers.\n",
    "centers = []\n",
    "for ann in annotation:\n",
    "    x, y, w, h = ann\n",
    "    centers.append((x+(w/2), y+(h/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53486145",
   "metadata": {},
   "source": [
    "Run the cell below as code if you want to see the centers being drawn in the image. The centers are red dots and they are pretty small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc4cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8e27388",
   "metadata": {},
   "source": [
    "draw = ImageDraw.Draw(im)\n",
    "for center in centers:\n",
    "    draw.point(center, fill=(255, 0, 0))\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c6529",
   "metadata": {},
   "source": [
    "Now we are going to focus on training and learning. We need to create a Gaussian Patch to help the model learn. Otherwise the model might have a hard time predicting the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a094e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch(scale=12):\n",
    "    \"\"\"Creates a heatmap using Gaussian Distribution.\"\"\"\n",
    "\n",
    "    # constants.\n",
    "    sigma = 1\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x_mesh, y_mesh = torch.meshgrid(torch.arange(0, 6*sigma+1, 1), torch.arange(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "    # the center of the gaussian patch should be 1\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    # generate this 7x7 gaussian patch\n",
    "    xmesh = torch.square(torch.sub(x_mesh, center_x))\n",
    "    ymesh = torch.square(torch.sub(y_mesh, center_y))\n",
    "    denom = (sigma**2) * 2\n",
    "    gaussian_patch = torch.mul(torch.exp(torch.div(torch.neg(torch.add(xmesh, ymesh)), denom)), scale)\n",
    "\n",
    "    return gaussian_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7d5ce",
   "metadata": {},
   "source": [
    "Display what the Gaussian Patch looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0e4d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2340980e-04, 1.5034392e-03, 6.7379470e-03, 1.1108996e-02,\n",
       "        6.7379470e-03, 1.5034392e-03, 1.2340980e-04],\n",
       "       [1.5034392e-03, 1.8315639e-02, 8.2084998e-02, 1.3533528e-01,\n",
       "        8.2084998e-02, 1.8315639e-02, 1.5034392e-03],\n",
       "       [6.7379470e-03, 8.2084998e-02, 3.6787945e-01, 6.0653067e-01,\n",
       "        3.6787945e-01, 8.2084998e-02, 6.7379470e-03],\n",
       "       [1.1108996e-02, 1.3533528e-01, 6.0653067e-01, 1.0000000e+00,\n",
       "        6.0653067e-01, 1.3533528e-01, 1.1108996e-02],\n",
       "       [6.7379470e-03, 8.2084998e-02, 3.6787945e-01, 6.0653067e-01,\n",
       "        3.6787945e-01, 8.2084998e-02, 6.7379470e-03],\n",
       "       [1.5034392e-03, 1.8315639e-02, 8.2084998e-02, 1.3533528e-01,\n",
       "        8.2084998e-02, 1.8315639e-02, 1.5034392e-03],\n",
       "       [1.2340980e-04, 1.5034392e-03, 6.7379470e-03, 1.1108996e-02,\n",
       "        6.7379470e-03, 1.5034392e-03, 1.2340980e-04]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, h = im.size\n",
    "x, y = centers[0]\n",
    "patch = generate_patch(1)\n",
    "torch.Tensor.numpy(patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5912c0e",
   "metadata": {},
   "source": [
    "Display it as an image."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d75bf7c3",
   "metadata": {},
   "source": [
    "# add a dimension. (Grayscale)\n",
    "patch = torch.unsqueeze(patch, dim=0)\n",
    "\n",
    "# convert to image.\n",
    "t = transforms.ToPILImage()\n",
    "patch_img = t(patch)\n",
    "# display.\n",
    "patch_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f12dde",
   "metadata": {},
   "source": [
    "The Gaussian Patch is being created properly as seen by the output tensor after converting from img to tensor. Now we should try to figure out how to place the patch in the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f053a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap(width, height, center_x, center_y, gau_patch):\n",
    "    \"\"\"Places a Gaussian Patch in the heatmap.\"\"\"\n",
    "\n",
    "    # constants.\n",
    "    heatmap = np.zeros((height, width))\n",
    "    sigma = 1\n",
    "    visibility = 2\n",
    "    gau_patch = torch.Tensor.numpy(gau_patch)\n",
    "\n",
    "    # this gaussian patch is 7x7, let's get four corners of it first\n",
    "    xmin = center_x - 3 * sigma\n",
    "    ymin = center_y - 3 * sigma\n",
    "    xmax = center_x + 3 * sigma\n",
    "    ymax = center_y + 3 * sigma\n",
    "    \n",
    "    # if outside the image don't include the gaussian patch.\n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    # determine boundaries for patch if outside the image.\n",
    "    patch_xmin = max(0, -xmin)\n",
    "    patch_ymin = max(0, -ymin)\n",
    "    patch_xmax = min(xmax, width) - xmin\n",
    "    patch_ymax = min(ymax, height) - ymin\n",
    "    \n",
    "    # we need to determine where to put this patch in the whole heatmap\n",
    "    heatmap_xmin = max(0, xmin)\n",
    "    heatmap_ymin = max(0, ymin)\n",
    "\n",
    "    for j in range(patch_ymin, patch_ymax):\n",
    "        for i in range(patch_xmin, patch_xmax):\n",
    "            heatmap[j+heatmap_ymin, i+heatmap_xmin] = gau_patch[j, i]\n",
    "\n",
    "    return torch.FloatTensor(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75934687",
   "metadata": {},
   "source": [
    "Before generating the final image for training (aka the heatmap), we are going to preprocess the data. To a 64x64 image which would be equivalent to the output shape of the HourGlass Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7342b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx, ry = 64/w, 64/h\n",
    "\n",
    "# re-locate the centers for the new image dimensions.\n",
    "resized_centers = []\n",
    "for center in centers:\n",
    "    cx, cy = center\n",
    "    resized_centers.append((cx*rx, cy*ry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b68804c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pos, y_pos = resized_centers[0]\n",
    "heatmap_example = make_heatmap(64, 64, int(x_pos), int(y_pos), patch)\n",
    "heatmap_example[int(y_pos)][int(x_pos)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752dc59",
   "metadata": {},
   "source": [
    "The patch was added since the center of the pedestrian has value 1. Run the code below if you want a visualization of the heatmap."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf0174f3",
   "metadata": {},
   "source": [
    "to_img = transforms.ToPILImage()\n",
    "heatmap_img = to_img(torch.unsqueeze(heatmap_example, dim=0))\n",
    "heatmap_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ff6f6",
   "metadata": {},
   "source": [
    "Now we need to create a function that will generate the appropiate set of heatmaps for multiple pedestrians in a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51a2359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap_plural(width, height, keypoints, gau_patch):\n",
    "    \"\"\"Places a Gaussian Patch in the heatmap.\"\"\"\n",
    "\n",
    "    # constants.\n",
    "    heatmap = np.zeros((height, width))\n",
    "    sigma = 1\n",
    "    visibility = 2\n",
    "    gau_patch = torch.Tensor.numpy(gau_patch)\n",
    "\n",
    "    # generates the limits of the patch for each keypoint.\n",
    "    coordinates = []\n",
    "    for keypoint in keypoints:\n",
    "        center_x, center_y = keypoint\n",
    "        \n",
    "        # get the coordinates.\n",
    "        xmin = center_x - 3 * sigma\n",
    "        ymin = center_y - 3 * sigma\n",
    "        xmax = center_x + 3 * sigma\n",
    "        ymax = center_y + 3 * sigma\n",
    "        coordinates.append((xmin, ymin, xmax, ymax))\n",
    "\n",
    "    # for each keypoint draw the patch.\n",
    "    for coordinate in coordinates:\n",
    "        # unpack the coordinates.\n",
    "        xmin, ymin, xmax, ymax = coordinate\n",
    "\n",
    "        # if outside the image don't include the patch.\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "            pass\n",
    "\n",
    "        # determine boundaries for patch if outside the image.\n",
    "        patch_xmin = max(0, -xmin)\n",
    "        patch_ymin = max(0, -ymin)\n",
    "        patch_xmax = min(xmax, width) - xmin\n",
    "        patch_ymax = min(ymax, height) - ymin\n",
    "\n",
    "        # we need to determine where to put this patch in the whole heatmap\n",
    "        heatmap_xmin = int(max(0, xmin))\n",
    "        heatmap_ymin = int(max(0, ymin))\n",
    "\n",
    "        # add the patches to the image.\n",
    "        for j in range(int(patch_ymin), int(patch_ymax)):\n",
    "            for i in range(int(patch_xmin), int(patch_xmax)):\n",
    "                \n",
    "                # get the value within the patch.\n",
    "                gau_pixel = gau_patch[j, i]\n",
    "                pixel = heatmap[j+heatmap_ymin, i+heatmap_xmin]\n",
    "                \n",
    "                # if the pixel already has a value assigned to it take the max.\n",
    "                if pixel > 0:\n",
    "                    heatmap[j+heatmap_ymin, i+heatmap_xmin] = max(pixel, gau_pixel)\n",
    "                else:\n",
    "                    heatmap[j+heatmap_ymin, i+heatmap_xmin] = gau_pixel\n",
    "\n",
    "    # return the final heatmap as a tensor.\n",
    "    return torch.FloatTensor(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450fcd3",
   "metadata": {},
   "source": [
    "Now we will create a heatmap which includes all the pedestrians within the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "804e0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new patch\n",
    "patch = generate_patch(1)\n",
    "sample_heatmap = make_heatmap_plural(64, 64, resized_centers, patch)\n",
    "sample_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9fd26b",
   "metadata": {},
   "source": [
    "To display the heatmap convert the cell below to a code cell and run."
   ]
  },
  {
   "cell_type": "raw",
   "id": "60858b52",
   "metadata": {},
   "source": [
    "to_img = transforms.ToPILImage()\n",
    "heatmap_sample_img = to_img(torch.unsqueeze(sample_heatmap, dim=0))\n",
    "heatmap_sample_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518a566",
   "metadata": {},
   "source": [
    "Now we will create the Torch dataset and load it. Then we will train the model and look at some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fae0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrwHeatMaps(Dataset):\n",
    "    \"\"\"Creates a custom dataset with the PRW data.\"\"\"\n",
    "\n",
    "    def __init__(self, img_path, ann_path, indexes, transform=None, resize_shape=(64, 64)):\n",
    "        \"\"\"Instantiates the dataset.\"\"\"\n",
    "\n",
    "        # setup transforms\n",
    "        self.transform = transform\n",
    "        self.resize_shape = resize_shape\n",
    "\n",
    "        # set up img and annotations paths.\n",
    "        self.img_path = img_path\n",
    "        self.ann_path = ann_path\n",
    "\n",
    "        # get all image and annotations.\n",
    "        self.img_names = sorted(list(os.listdir(img_path)))\n",
    "        self.ann_names = sorted(list(os.listdir(ann_path)))\n",
    "        \n",
    "        # get the given index range. (This is for train/testing)\n",
    "        self.img_names = itemgetter(*indexes)(self.img_names)\n",
    "        self.ann_names = itemgetter(*indexes)(self.ann_names)\n",
    "        \n",
    "        # create a patch.\n",
    "        self.patch = generate_patch(1)\n",
    "\n",
    "        # check that the annotations and images match.\n",
    "        if len(self.img_names) != len(self.ann_names):\n",
    "            raise ValueError(\"Images and annotations don't align\")\n",
    "        for img, ann in zip(self.img_names, self.ann_names):\n",
    "            name, _ = os.path.splitext(img)\n",
    "            if name not in ann:\n",
    "                raise ValueError(\"Image and annotation names don't align\")\n",
    "\n",
    "    def _get_ann(self, name):\n",
    "        \"\"\"Loads in the annotation and gets the bounding boxes.\"\"\"\n",
    "\n",
    "        # load in the annotation.\n",
    "        name = os.path.join(self.ann_path, name)\n",
    "        ann = loadmat(name)\n",
    "        if 'box_new' in ann.keys():\n",
    "            ann = ann['box_new']\n",
    "        elif 'anno_file' in ann.keys():\n",
    "            ann = ann['anno_file']\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Annotation Error\")\n",
    "            \n",
    "        # remove the first value of each set of bounding boxes.\n",
    "        ann = [a[1:] for a in ann]\n",
    "\n",
    "        centers = list()\n",
    "        # calculate the center of the bounding box.\n",
    "        for a in ann:\n",
    "            # unpack the annotation.\n",
    "            x, y, w, h = a\n",
    "            centers.append((x+(w/2), y+(h/2)))\n",
    "\n",
    "        return centers\n",
    "\n",
    "    def _get_img(self, name):\n",
    "        \"\"\"Loads in an image.\"\"\"\n",
    "\n",
    "        name = os.path.join(self.img_path, name)\n",
    "        photo = cv.cvtColor(cv.imread(name), cv.COLOR_BGR2RGB)\n",
    "        height, width = photo.shape[0], photo.shape[1]\n",
    "\n",
    "        return photo, width, height\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples\"\"\"\n",
    "\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one sample of the data.\"\"\"\n",
    "\n",
    "        # get the image and the corresponding annotation.\n",
    "        img, width, height = self._get_img(self.img_names[index])\n",
    "        ann = self._get_ann(self.ann_names[index])\n",
    "        count = len(ann)\n",
    "\n",
    "        # convert the annotation to tensor.\n",
    "        # ann = torch.FloatTensor(ann)\n",
    "\n",
    "        # apply transform if available.\n",
    "        if self.transform:\n",
    "            # get the center of mass.\n",
    "            rx, ry = self.resize_shape[0]/width, self.resize_shape[1]/height\n",
    "            ann = [(a[0]*rx, a[1]*ry) for a in ann]\n",
    "\n",
    "            # create the patch and the heatmap.\n",
    "            heat = make_heatmap_plural(self.resize_shape[0],\n",
    "                                       self.resize_shape[1],\n",
    "                                       ann, \n",
    "                                       self.patch)\n",
    "\n",
    "            return self.transform(img), heat, count\n",
    "\n",
    "        return img, ann, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "136dd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std was calculated in another notebook. \n",
    "MEAN = [0.5062, 0.5004, 0.4894]\n",
    "STD = [0.1942, 0.1879, 0.1980]\n",
    "\n",
    "# create the transform.\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# create the dataset.\n",
    "data = PrwHeatMaps(r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames',\n",
    "                   r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations', \n",
    "                   list(range(0, 10)), preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78198b96",
   "metadata": {},
   "source": [
    "We will look at a single datapoint and what it consists of. It should be an image as a tensor and a heatmap as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c060d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.5427,  2.5427,  2.5225,  ...,  0.0791,  0.1195,  0.0993],\n",
       "          [ 2.5427,  2.5427,  2.5225,  ...,  0.0791,  0.0388,  0.0589],\n",
       "          [ 2.5427,  2.5427,  2.5225,  ...,  0.0589,  0.0186, -0.0218],\n",
       "          ...,\n",
       "          [ 0.0388,  0.0589,  0.0186,  ..., -0.8700, -0.7488, -0.7690],\n",
       "          [ 0.0186,  0.0388, -0.0420,  ..., -0.7488, -0.7690, -0.7690],\n",
       "          [-0.0824, -0.1026, -0.1632,  ..., -0.8700, -0.7892, -0.7488]],\n",
       " \n",
       "         [[ 2.6589,  2.6589,  2.6380,  ...,  0.1127,  0.1544,  0.1335],\n",
       "          [ 2.6589,  2.6589,  2.6380,  ...,  0.1127,  0.0709,  0.0918],\n",
       "          [ 2.6589,  2.6589,  2.6380,  ...,  0.0918,  0.0500,  0.0083],\n",
       "          ...,\n",
       "          [-0.0126, -0.0126,  0.0083,  ..., -0.9100, -0.8056, -0.9100],\n",
       "          [-0.0126,  0.0292, -0.0126,  ..., -0.8056, -0.8265, -0.8683],\n",
       "          [-0.0960, -0.0960, -0.1378,  ..., -0.9100, -0.8683, -0.8265]],\n",
       " \n",
       "         [[ 2.5788,  2.5788,  2.5590,  ...,  0.1625,  0.2021,  0.1823],\n",
       "          [ 2.5788,  2.5788,  2.5590,  ...,  0.1625,  0.1229,  0.1427],\n",
       "          [ 2.5788,  2.5788,  2.5590,  ...,  0.1427,  0.1031,  0.0634],\n",
       "          ...,\n",
       "          [ 0.0436,  0.1031,  0.0832,  ..., -0.7882, -0.6892, -0.7684],\n",
       "          [ 0.0832,  0.0634,  0.0238,  ..., -0.6892, -0.7090, -0.7486],\n",
       "          [ 0.0238, -0.0554, -0.0950,  ..., -0.7882, -0.7486, -0.7090]]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a look at a datapoint\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa301f",
   "metadata": {},
   "source": [
    "Run the cell below to get a look at the heatmap. The image itself is resized and the channel intensities are standardized using the Datasets mean and standard deviation. The heatmap is the same as in the tests above."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3af51b9c",
   "metadata": {},
   "source": [
    "to_img = transforms.ToPILImage()\n",
    "sample = to_img(torch.unsqueeze(data[0][1], dim=0))\n",
    "sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d0b584b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7abc6",
   "metadata": {},
   "source": [
    "Now we will create a train test split for out model. Given how the data is loading into torch we need to create a list of indexes that will serve as training and testing. We also test to see if the indexes did indeed change, python cell should print false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c1bd802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get image directory length.\n",
    "num_images = len(os.listdir(r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames'))\n",
    "\n",
    "# generate all possible indexes and shuffle them.\n",
    "indexes = list(range(num_images))\n",
    "indexes_2 = list(range(num_images))\n",
    "random.seed(13)\n",
    "random.shuffle(indexes)\n",
    "print(indexes == indexes_2)\n",
    "\n",
    "# delete and collect.\n",
    "del indexes_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0582d4",
   "metadata": {},
   "source": [
    "We are going to use a 70/30 split. So we will get the first 70% of the indexes and the use the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08bfe18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingSet Size: 8271, TestingSet Size: 3545\n"
     ]
    }
   ],
   "source": [
    "# get 80% of the total number of image and split.\n",
    "max_id = int(num_images * 0.7)\n",
    "train_idx = indexes[:max_id]\n",
    "test_idx = indexes[max_id:]\n",
    "print(f'TrainingSet Size: {len(train_idx)}, TestingSet Size: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf36fc",
   "metadata": {},
   "source": [
    "This might not be enough data to train a fully functional model. But we will see after training. Next we will be creating the data loaders and setting some of the parameters. You might notice the use of prw.PrwHeatMaps(...), multiprocessing was having issues with Jupyter and how it uses mutiple workers in a multi thread process; hence, the custom dataset was moved to a Python file and imported into the notebook. This fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c058cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths to the data.\n",
    "images_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames'\n",
    "annota_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations'\n",
    "batch = 32\n",
    "\n",
    "# create the datasets.\n",
    "train_set = prw.PrwHeatMaps(images_path, annota_path, train_idx, preprocess)\n",
    "test_set = prw.PrwHeatMaps(images_path, annota_path, test_idx, preprocess)\n",
    "\n",
    "# create data loaders\n",
    "trainLoader = DataLoader(train_set, batch_size=batch, shuffle=True, num_workers=os.cpu_count())\n",
    "testLoader = DataLoader(test_set, batch_size=batch, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330298d",
   "metadata": {},
   "source": [
    "For the loss function we will use Mean Error Squared as in the paper [Stacked Hourglass Networks for Human Estimation](https://arxiv.org/pdf/1603.06937.pdf). As mentioned earlier, we do add a Gaussian Patch to the images, which should go well with using Mean Error Squared as the loss function. The Gaussian Patch draws a region of interest, so we can mesure how off (using MSE) are the intesities in the output of the model. We use Adam optimizer for our cost minimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a405250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the learning rate.\n",
    "LR = 0.0001\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_set) // batch\n",
    "valSteps = len(test_set) // batch\n",
    "\n",
    "# set the loss and optimizer.\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f84bad",
   "metadata": {},
   "source": [
    "We should have an instance of the HourGlass Model from earlier. Lets put that in train mode and set up the required train loss informormation. We are also going to add a few keys in a diuctionary to track error. One of those errors will be the pedestrian count in the image. Since we are looking at heatmap. we are just going to get the peaks within the image and compare. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f20c4",
   "metadata": {},
   "source": [
    "This next cell is the complete training and validation loop. Since we are using a Custom Dataset and loss function, we need to manually generate the loop and the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fabaed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 6.\nOriginal Traceback (most recent call last):\n  File \"/Users/chiahaohsutai/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/chiahaohsutai/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/chiahaohsutai/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/chiahaohsutai/Documents/GitHub/PedestrianDetection/custom/datasetprw.py\", line 166, in __getitem__\n    ann = self._get_ann(self.ann_names[index])\n  File \"/Users/chiahaohsutai/Documents/GitHub/PedestrianDetection/custom/datasetprw.py\", line 134, in _get_ann\n    raise ValueError(\"Invalid Annotation Error\")\nValueError: Invalid Annotation Error\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# loop over the validation set\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, truth_map, count \u001b[38;5;129;01min\u001b[39;00m testLoader:\n\u001b[1;32m     65\u001b[0m     \n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# send the input to the device\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     img, truth_map \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(DEVICE), truth_map\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# make the predictions and calculate the validation loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1326\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1325\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 6.\nOriginal Traceback (most recent call last):\n  File \"/Users/chiahaohsutai/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/chiahaohsutai/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/chiahaohsutai/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/chiahaohsutai/Documents/GitHub/PedestrianDetection/custom/datasetprw.py\", line 166, in __getitem__\n    ann = self._get_ann(self.ann_names[index])\n  File \"/Users/chiahaohsutai/Documents/GitHub/PedestrianDetection/custom/datasetprw.py\", line 134, in _get_ann\n    raise ValueError(\"Invalid Annotation Error\")\nValueError: Invalid Annotation Error\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# create a dictionary that stores the training loss.\n",
    "H = {\"total_train_loss\": [], \"total_val_loss\": [], \n",
    "     \"total_train_acc\": [], \"total_validation_acc\": []}\n",
    "\n",
    "# indicate training.\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "# record current time for total training time later.\n",
    "startTime = time.time()\n",
    "\n",
    "# loop over epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    \n",
    "    # initialize the number of correct predictions in the training\n",
    "    # and validation step\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "    \n",
    "    # loop over the training set\n",
    "    for img, truth_map, count in trainLoader:\n",
    "        # send the input to the device\n",
    "        img, truth_map = img.to(DEVICE), truth_map.to(DEVICE)\n",
    "        \n",
    "        \n",
    "        # perform a forward pass and calculate the training loss\n",
    "        predictions = model(img)\n",
    "        loss = loss_fn(predictions, truth_map)\n",
    "        totalLoss = loss\n",
    "        \n",
    "        # zero out the gradients, perform the backpropagation step,\n",
    "        # and update the weights\n",
    "        opt.zero_grad()\n",
    "        totalLoss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # add the loss to the total training loss so far and\n",
    "        # calculate the number of correct predictions\n",
    "        totalTrainLoss += totalLoss\n",
    "        \n",
    "        flat_count = torch.flatten(predictions.detach(), start_dim=1)\n",
    "        for idx in range(flat_count.shape[0]):\n",
    "            \n",
    "            # get the count for the tensor and add to acc. count.\n",
    "            pred_count = torch.sum(flat_count[idx] >= 1.0)\n",
    "            if (pred_count - count[idx]) == 0:\n",
    "                trainCorrect += 1\n",
    "    \n",
    "    # switch off autograd\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # set the model in evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # loop over the validation set\n",
    "        for img, truth_map, count in testLoader:\n",
    "            \n",
    "            # send the input to the device\n",
    "            img, truth_map = img.to(DEVICE), truth_map.to(DEVICE)\n",
    "            \n",
    "            # make the predictions and calculate the validation loss\n",
    "            predictions = model(img)\n",
    "            loss = loss_fn(predictions, truth_map)\n",
    "            totalLoss = loss\n",
    "            totalValLoss += totalLoss\n",
    "            \n",
    "            flat_count = torch.flatten(predictions.detach(), start_dim=1)\n",
    "            for idx in range(flat_count.shape[0]):\n",
    "\n",
    "                # get the count for the tensor and add to acc. count.\n",
    "                pred_count = torch.sum(flat_count[idx] >= 1)\n",
    "                if (pred_count - count[idx]) == 0:\n",
    "                    valCorrect += 1\n",
    "    \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "    \n",
    "    # calculate the training and validation accuracy\n",
    "    trainCorrect = trainCorrect / len(train_set)\n",
    "    valCorrect = valCorrect / len(test_set)\n",
    "    \n",
    "    # update our training history\n",
    "    H[\"total_train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"total_train_acc\"].append(trainCorrect)\n",
    "    H[\"total_val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "    H[\"total_validation_acc\"].append(valCorrect)\n",
    "    \n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, epochs))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\".format(avgValLoss, valCorrect))\n",
    "    \n",
    "# print out the time it took for the epoch to train.\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24dcda8",
   "metadata": {},
   "source": [
    "Issue with the count value. Check it and also update independet script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to current folder.\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "torch.save(model, curr_dir)\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"total_train_loss\"], label=\"total_train_loss\")\n",
    "plt.plot(H[\"total_val_loss\"], label=\"total_val_loss\")\n",
    "plt.plot(H[\"train_class_acc\"], label=\"train_class_acc\")\n",
    "plt.plot(H[\"val_class_acc\"], label=\"val_class_acc\")\n",
    "plt.title(\"Total Training Loss and Classification Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# save the training plot\n",
    "plotPath = os.path.sep.join([curr_dir, \"training.png\"])\n",
    "plt.savefig(plotPath)\n",
    "\n",
    "# show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4033a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
