{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bdabc5",
   "metadata": {},
   "source": [
    "### Faster RCNN \n",
    "\n",
    "We are looking into the Faster RCNN model with a mobile v3 backbone, which is supposed to be faster than the Faster RCNN model with ResNet backbone. However, its supposed to be less accurate.\n",
    "\n",
    "We are following a similar or equivalent procedure as the other notebook with an RCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64840792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import FasterRCNN_MobileNet_V3_Large_320_FPN_Weights\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os import path\n",
    "import numpy as np\n",
    "from torch import device\n",
    "from torch import cuda\n",
    "import cv2\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66874eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "join = path.join\n",
    "\n",
    "# image path and annotations path.\n",
    "img_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/images/frames'\n",
    "ann_path = r'/Users/chiahaohsutai/Documents/GitHub/PRW/annotations'\n",
    "\n",
    "# get the image names.\n",
    "img_names = sorted(list(listdir(img_path)))\n",
    "img_names = [join(img_path, name) for name in img_names]\n",
    "\n",
    "# get the annoation names.\n",
    "ann_names = sorted(list(listdir(ann_path)))\n",
    "ann_names = [join(ann_path, name) for name in ann_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a02b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# define the torchvision image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def predict(image, model, device, detection_threshold):\n",
    "\n",
    "    # transform the image to tensor.\n",
    "    image = transform(image).to(device)\n",
    "    image = image.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(image)     # get the predictions on the image\n",
    "\n",
    "    # get all the predicited class names\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "\n",
    "    # get score for all the predicted objects.\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "    # get all the predicted bounding boxes.\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "    # get boxes above the threshold score.\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "    labels = pred_labels[pred_scores >= detection_threshold]\n",
    "\n",
    "    return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce32df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, labels, image):\n",
    "\n",
    "    # create a color for the bounding box.\n",
    "    COLOR = [255, 0, 0] \n",
    "  \n",
    "    # read the image with OpenCV\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # draw only the boxes which are persons.\n",
    "    for i, box in enumerate(boxes):\n",
    "        if labels[i] == 1:\n",
    "            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), COLOR, 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef67e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model.\n",
    "weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f37fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device.\n",
    "device = device('mps' if torch.has_mps else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1120dcde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a prediction.\n",
    "image = Image.open(img_names[0])\n",
    "model.eval().to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa4ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    boxes, labels = predict(image, model, device, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d0fad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5664ddee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = draw_boxes(boxes, labels, image)\n",
    "\n",
    "# display the image.\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyWindow('Image')\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132dd716",
   "metadata": {},
   "source": [
    "This model performs significantly worst than the FasterRCNN with ResNet 50 backbone but its much much faster. It delivers a results extremely quickly. But it only catches pedestrians which are in clear sight and close to the front of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49905f55",
   "metadata": {},
   "source": [
    "Now we will do some evalutions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c161a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002cdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the `Detection` object\n",
    "Detection = namedtuple(\"Detection\", [\"image_path\", \"gt\", \"pred\"])\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"Calculates Intersection Over Union.\"\"\"\n",
    "    \n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa361af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(ann):\n",
    "    \"\"\"Gets the correct key for annotations.\"\"\"\n",
    "    \n",
    "    keys = ann.keys()\n",
    "    key = None\n",
    "    \n",
    "    # get the correct key for the bouding box.\n",
    "    for k in ['box_new', 'anno_file', 'anno_previous']:\n",
    "        if k in keys:\n",
    "            key = k\n",
    "            break\n",
    "    if key is None:\n",
    "        raise ValueError(\"Invalid Annotation Error\")\n",
    "    \n",
    "    # get the bounding boxes and convert to coordinates.\n",
    "    bbox = [box[1:] for box in ann[key]]\n",
    "    for box in bbox:\n",
    "        xmin, ymin, w, h = box\n",
    "        xmax, ymax = xmin+w, ymin+h\n",
    "        box[-2] = xmax\n",
    "        box[-1] = ymax\n",
    "        \n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a3fb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iou = []\n",
    "# truth, pred\n",
    "total_count = []\n",
    "total = len(img_names)\n",
    "\n",
    "# evaluate the model.\n",
    "for img, ann in zip(img_names, ann_names):\n",
    "    data = loadmat(ann)\n",
    "    photo = Image.open(img)\n",
    "    gt = get_boxes(data)\n",
    "    \n",
    "    # get a prediction\n",
    "    with torch.no_grad():\n",
    "        pred = predict(photo, model, device, 0.7)\n",
    "    \n",
    "    # go through the prediciton and calulate iou\n",
    "    boxes, labels = pred\n",
    "    for bbox, label in zip(boxes, labels):\n",
    "        union = 0\n",
    "        if int(label) == 1:\n",
    "            for b in gt:\n",
    "                union = max(union, iou(b, bbox))\n",
    "        if union > 0.25:\n",
    "            total_iou += union\n",
    "    \n",
    "    # get a count of the boxes.\n",
    "    total_count.append((len(gt), len([label == 1 for label in pred[-1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e31d985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "11816\n"
     ]
    }
   ],
   "source": [
    "print(len(total_iou))\n",
    "print(len(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee1856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
